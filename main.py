import feedparser
import os
import time
import re
import requests
import json
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium_stealth import stealth

# --- Ø¨Ø±Ù…Ø¬Ø© ahmed si (ØªÙ… Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø¨Ø¢Ù„ÙŠØ© Ø±ÙØ¹ Ø§Ù„ØµÙˆØ± Ø¨ÙˆØ§Ø³Ø·Ø© Gemini v22.6) ---

RSS_URL = "https://Fastyummyfood.com/feed"
POSTED_LINKS_FILE = "posted_links.txt"
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")

def get_posted_links():
    if not os.path.exists(POSTED_LINKS_FILE): return set()
    with open(POSTED_LINKS_FILE, "r", encoding='utf-8') as f: return set(line.strip() for line in f)

def add_posted_link(link):
    with open(POSTED_LINKS_FILE, "a", encoding='utf-8') as f: f.write(link + "\n")

def get_next_post_to_publish():
    print(f"--- 1. Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ù‚Ø§Ù„Ø§Øª ÙÙŠ: {RSS_URL}")
    feed = feedparser.parse(RSS_URL)
    if not feed.entries: return None
    print(f"--- ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(feed.entries)} Ù…Ù‚Ø§Ù„Ø§Øª.")
    posted_links = get_posted_links()
    for entry in reversed(feed.entries):
        if entry.link not in posted_links:
            print(f">>> ØªÙ… ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù‚Ø§Ù„: {entry.title}")
            return entry
    return None

def scrape_images_from_article(url, driver):
    print(f"--- ğŸ–¼ï¸ Ø¬Ø§Ø±ÙŠ ÙƒØ´Ø· Ø§Ù„ØµÙˆØ± Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø£ØµÙ„ÙŠ: {url}")
    image_urls = []
    try:
        driver.get(url)
        wait = WebDriverWait(driver, 20)
        content_area = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "article.article")))
        images = content_area.find_elements(By.TAG_NAME, "img")
        print(f"--- ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(images)} ØµÙˆØ±Ø© ÙÙŠ Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰.")
        for img in images:
            src = img.get_attribute('src')
            if src and src.startswith('http') and not "data:image" in src:
                if src not in image_urls:
                    image_urls.append(src)
            if len(image_urls) == 2:
                break
        if image_urls:
            print(f"--- âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(image_urls)} Ø±ÙˆØ§Ø¨Ø· ØµÙˆØ± Ø¨Ù†Ø¬Ø§Ø­.")
        else:
            print("--- âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØµÙˆØ± Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù† Ø§Ù„ØµÙØ­Ø©.")
        return image_urls
    except Exception as e:
        print(f"!!! Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ÙƒØ´Ø· Ø§Ù„ØµÙˆØ±: {e}")
        return []

def rewrite_content_with_gemini(title, content_html, original_link, image_urls):
    if not GEMINI_API_KEY:
        print("!!! ØªØ­Ø°ÙŠØ±: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…ÙØªØ§Ø­ GEMINI_API_KEY.")
        return None
    print("--- ğŸ’¬ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Gemini API Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù‚Ø§Ù„ Ø§Ø­ØªØ±Ø§ÙÙŠ...")
    clean_content = re.sub('<[^<]+?>', ' ', content_html)
    prompt = f"""
    You are a professional SEO copywriter for Medium. Your task is to take an original recipe title and content, and write a full Medium-style article (around 600 words) optimized for SEO, engagement, and backlinks.
    **Original Data:**
    - Original Title: "{title}"
    - Original Content Snippet: "{clean_content[:1500]}"
    - Link to the full recipe: "{original_link}"
    - Available Image URLs: "{', '.join(image_urls) if image_urls else 'None'}"
    **Article Requirements:**
    1.  **Title:** Create a new, SEO-optimized title using the Hybrid Headline strategy.
    2.  **Article Body (HTML Format):**
        - Write a 600-700 word article in clean HTML.
        - **Image Placement:** Crucially, you MUST insert two image placeholders exactly as written below:
            - `<!-- IMAGE 1 PLACEHOLDER -->` after the intro.
            - `<!-- IMAGE 2 PLACEHOLDER -->` before the listicle section or a relevant paragraph.
            Do not add your own `<img>` tags.
    3.  **Smart Closing Method:** End with a compelling call-to-action that encourages readers to visit the original recipe link for detailed instructions.
    **Output Format:**
    Return ONLY a valid JSON object with the keys: "new_title", "new_html_content", "tags", and "alt_texts". "alt_texts" should be an array of two descriptive strings for the images.
    """
    api_url = f'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={GEMINI_API_KEY}'
    headers = {'Content-Type': 'application/json'}
    data = {"contents": [{"parts": [{"text": prompt}]}], "generationConfig": {"maxOutputTokens": 4096}}
    try:
        response = requests.post(api_url, headers=headers, data=json.dumps(data), timeout=180)
        response.raise_for_status()
        response_json = response.json()
        raw_text = response_json['candidates'][0]['content']['parts'][0]['text']
        json_match = re.search(r'\{.*\}', raw_text, re.DOTALL)
        if json_match:
            clean_json_str = json_match.group(0)
            result = json.loads(clean_json_str)
            print("--- âœ… ØªÙ… Ø§Ø³ØªÙ„Ø§Ù… Ù…Ù‚Ø§Ù„ ÙƒØ§Ù…Ù„ Ù…Ù† Gemini.")
            return {"title": result.get("new_title", title), "content": result.get("new_html_content", content_html), "tags": result.get("tags", []), "alt_texts": result.get("alt_texts", ["Image of the dish", "Another view of the recipe"])}
        else:
            raise ValueError("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØµÙŠØºØ© JSON ÙÙŠ Ø±Ø¯ Gemini.")
    except Exception as e:
        print(f"!!! Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Gemini: {e}")
        return None

# --- Ø¯Ø§Ù„Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© ---
def download_image(url, filename):
    try:
        response = requests.get(url, stream=True)
        response.raise_for_status()
        with open(filename, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        return True
    except Exception as e:
        print(f"!!! ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© {url}: {e}")
        return False

# --- Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„ØµÙ‚ Ù…Ø­ØªÙˆÙ‰ HTML ---
def paste_html(driver, html_content):
    if not html_content.strip(): return
    js_script = "const html = arguments[0]; const sel = window.getSelection(); if (sel.rangeCount > 0) { const range = sel.getRangeAt(0); range.deleteContents(); const el = document.createElement('div'); el.innerHTML = html; const frag = document.createDocumentFragment(); let node; while ((node = el.firstChild)) { frag.appendChild(node); } range.insertNode(frag); }"
    driver.execute_script(js_script, html_content)

def main():
    print("--- Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±ÙˆØ¨ÙˆØª Ø§Ù„Ù†Ø§Ø´Ø± v22.6 (Ù…Ø¹ Ø±ÙØ¹ Ø§Ù„ØµÙˆØ±) ---")
    
    options = webdriver.ChromeOptions()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("window-size=1920,1080")
    service = ChromeService(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    stealth(driver, languages=["en-US", "en"], vendor="Google Inc.", platform="Win32", webgl_vendor="Intel Inc.", renderer="Intel Iris OpenGL Engine", fix_hairline=True)
    
    downloaded_files = [] # Ù‚Ø§Ø¦Ù…Ø© Ù„ØªØªØ¨Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¤Ù‚ØªØ©
    try:
        post_to_publish = get_next_post_to_publish()
        if not post_to_publish:
            print(">>> Ø§Ù„Ù†ØªÙŠØ¬Ø©: Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù‚Ø§Ù„Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù†Ø´Ø±Ù‡Ø§.")
            return

        original_title = post_to_publish.title
        original_link = post_to_publish.link
        scraped_image_urls = scrape_images_from_article(original_link, driver)
        
        original_content_html = ""
        if 'content' in post_to_publish and post_to_publish.content:
            original_content_html = post_to_publish.content[0].value
        else:
            original_content_html = post_to_publish.summary

        rewritten_data = rewrite_content_with_gemini(original_title, original_content_html, original_link, scraped_image_urls)
        
        if not rewritten_data:
            print("!!! ÙØ´Ù„ GeminiØŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©.")
            return

        final_title = rewritten_data["title"]
        generated_html_content = rewritten_data["content"]
        ai_tags = rewritten_data.get("tags", [])
        
        # --- ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø­Ø³Ø¨ Ø£Ù…Ø§ÙƒÙ† Ø§Ù„ØµÙˆØ± ---
        parts = re.split(r'<!-- IMAGE (?:1|2) PLACEHOLDER -->', generated_html_content)
        part1 = parts[0] if len(parts) > 0 else ""
        part2 = parts[1] if len(parts) > 1 else ""
        part3 = parts[2] if len(parts) > 2 else ""

        sid_cookie = os.environ.get("MEDIUM_SID_COOKIE")
        uid_cookie = os.environ.get("MEDIUM_UID_COOKIE")
        if not sid_cookie or not uid_cookie:
            print("!!! Ø®Ø·Ø£: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙˆÙƒÙŠØ².")
            return

        print("--- 2. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¬Ù„Ø³Ø©...")
        driver.get("https://medium.com/")
        driver.add_cookie({"name": "sid", "value": sid_cookie, "domain": ".medium.com"})
        driver.add_cookie({"name": "uid", "value": uid_cookie, "domain": ".medium.com"})
        
        print("--- 3. Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ø¥Ù„Ù‰ Ù…Ø­Ø±Ø± Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª...")
        driver.get("https://medium.com/new-story")
        wait = WebDriverWait(driver, 30)

        print("--- 4. ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¹Ù†ÙˆØ§Ù† ÙˆØ§Ù„Ù…Ø­ØªÙˆÙ‰...")
        title_field = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'h3[data-testid="editorTitleParagraph"]')))
        title_field.click()
        title_field.send_keys(final_title)

        story_field = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'p[data-testid="editorParagraphText"]')))
        story_field.click()
        
        # --- Ø¢Ù„ÙŠØ© Ø§Ù„Ù†Ø´Ø± Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ---
        paste_html(driver, part1)
        story_field.send_keys(Keys.ENTER)

        # Ø±ÙØ¹ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰
        if scraped_image_urls:
            print("--- ğŸ“¥ ØªØ­Ù…ÙŠÙ„ ÙˆØ±ÙØ¹ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰...")
            image_path = "temp_image_1.jpg"
            if download_image(scraped_image_urls[0], image_path):
                downloaded_files.append(image_path)
                file_input = driver.find_element(By.CSS_SELECTOR, 'input[type="file"]')
                file_input.send_keys(os.path.abspath(image_path))
                # Ø§Ù†ØªØ¸Ø± Ø­ØªÙ‰ ÙŠØªÙ… Ø±ÙØ¹ Ø§Ù„ØµÙˆØ±Ø© (Ø¸Ù‡ÙˆØ±Ù‡Ø§ ÙÙŠ Ø§Ù„Ù…Ø­Ø±Ø±)
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'img[src^="https://miro.medium.com"]')))
                print("--- âœ… ØªÙ… Ø±ÙØ¹ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰.")
                time.sleep(3) # Ø§Ù†ØªØ¸Ø± Ù‚Ù„ÙŠÙ„Ø§Ù‹ Ù„Ù„ØªØ£ÙƒØ¯
        
        story_field.send_keys(Keys.ENTER)
        paste_html(driver, part2)
        story_field.send_keys(Keys.ENTER)

        # Ø±ÙØ¹ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©
        if len(scraped_image_urls) > 1:
            print("--- ğŸ“¥ ØªØ­Ù…ÙŠÙ„ ÙˆØ±ÙØ¹ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©...")
            image_path = "temp_image_2.jpg"
            if download_image(scraped_image_urls[1], image_path):
                downloaded_files.append(image_path)
                file_input = driver.find_element(By.CSS_SELECTOR, 'input[type="file"]')
                file_input.send_keys(os.path.abspath(image_path))
                # Ø§Ù†ØªØ¸Ø± Ø±ÙØ¹ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©
                wait.until(EC.presence_of_element_located((By.XPATH, '(//img[starts-with(@src, "https://miro.medium.com")])[2]')))
                print("--- âœ… ØªÙ… Ø±ÙØ¹ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©.")
                time.sleep(3)
        
        story_field.send_keys(Keys.ENTER)
        paste_html(driver, part3)
        time.sleep(5)

        # --- Ø§Ø³ØªÙƒÙ…Ø§Ù„ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ù†Ø´Ø± ÙƒØ§Ù„Ù…Ø¹ØªØ§Ø¯ ---
        print("--- 5. Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ù†Ø´Ø±...")
        publish_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'button[data-action="show-prepublish"]')))
        publish_button.click()

        print("--- 6. Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙˆØ³ÙˆÙ… Ø§Ù„Ù…ØªØ§Ø­Ø©...")
        if ai_tags:
            tags_input = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div[data-testid="publishTopicsInput"]')))
            tags_input.click()
            for tag in ai_tags[:5]:
                tags_input.send_keys(tag)
                time.sleep(0.5)
                tags_input.send_keys(Keys.ENTER)
                time.sleep(1)
            print(f"--- ØªÙ…Øª Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙˆØ³ÙˆÙ…: {', '.join(ai_tags[:5])}")
        else:
            print("--- Ù„Ø§ ØªÙˆØ¬Ø¯ ÙˆØ³ÙˆÙ… Ù„Ø¥Ø¶Ø§ÙØªÙ‡Ø§.")

        print("--- 7. Ø¥Ø±Ø³Ø§Ù„ Ø£Ù…Ø± Ø§Ù„Ù†Ø´Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ...")
        publish_now_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'button[data-testid="publishConfirmButton"]')))
        time.sleep(2)
        driver.execute_script("arguments[0].click();", publish_now_button)

        print("--- 8. Ø§Ù†ØªØ¸Ø§Ø± Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„Ø³Ù…Ø§Ø­ Ø¨Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Ø´Ø±...")
        time.sleep(15)

        add_posted_link(post_to_publish.link)
        print(">>> ğŸ‰ğŸ‰ğŸ‰ ØªÙ… Ù†Ø´Ø± Ø§Ù„Ù…Ù‚Ø§Ù„ Ø¨Ù†Ø¬Ø§Ø­! ğŸ‰ğŸ‰ğŸ‰")

    except Exception as e:
        print(f"!!! Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­: {e}")
        driver.save_screenshot("error_screenshot.png")
        with open("error_page_source.html", "w", encoding="utf-8") as f: f.write(driver.page_source)
        raise e
    finally:
        # --- ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¤Ù‚ØªØ© ---
        for f in downloaded_files:
            if os.path.exists(f):
                os.remove(f)
                print(f"--- ØªÙ… Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª: {f}")
        driver.quit()
        print("--- ØªÙ… Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø±ÙˆØ¨ÙˆØª ---")

if __name__ == "__main__":
    main()
