import feedparser
import os
import time
import re
import requests
import json
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium_stealth import stealth

# --- Ø¨Ø±Ù…Ø¬Ø© ahmed si - Ø§Ù„Ù†Ø³Ø®Ø© v28 Ù…Ø¹ Alt Text ---

# ====== Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…ÙˆÙ‚Ø¹ - ØºÙŠÙ‘Ø± Ù‡Ù†Ø§ ÙÙ‚Ø· ======
SITE_NAME = "Fastyummyfood"  # Ø§Ø³Ù… Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø¨Ø¯ÙˆÙ† .com
SITE_DOMAIN = f"{SITE_NAME}.com"
RSS_URL = f"https://{SITE_DOMAIN}/feed"
# ==========================================

POSTED_LINKS_FILE = "posted_links.txt"
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")

def get_posted_links():
    if not os.path.exists(POSTED_LINKS_FILE): return set()
    with open(POSTED_LINKS_FILE, "r", encoding='utf-8') as f: return set(line.strip() for line in f)

def add_posted_link(link):
    with open(POSTED_LINKS_FILE, "a", encoding='utf-8') as f: f.write(link + "\n")

def get_next_post_to_publish():
    print(f"--- 1. Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ù‚Ø§Ù„Ø§Øª ÙÙŠ: {RSS_URL}")
    feed = feedparser.parse(RSS_URL)
    if not feed.entries: return None
    print(f"--- ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(feed.entries)} Ù…Ù‚Ø§Ù„Ø§Øª.")
    posted_links = get_posted_links()
    for entry in reversed(feed.entries):
        if entry.link not in posted_links:
            print(f">>> ØªÙ… ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù‚Ø§Ù„: {entry.title}")
            return entry
    return None

def extract_image_url_from_entry(entry):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙˆÙ„ ØµÙˆØ±Ø© Ù…Ù† RSS feed"""
    if hasattr(entry, 'media_content') and entry.media_content:
        for media in entry.media_content:
            if 'url' in media and media.get('medium') == 'image': return media['url']
    if hasattr(entry, 'enclosures') and entry.enclosures:
        for enclosure in entry.enclosures:
            if 'href' in enclosure and 'image' in enclosure.get('type', ''): return enclosure.href
    content_html = ""
    if 'content' in entry and entry.content: content_html = entry.content[0].value
    else: content_html = entry.summary
    match = re.search(r'<img[^>]+src="([^">]+)"', content_html)
    if match: return match.group(1)
    return None

def is_valid_article_image(url):
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„ØµÙˆØ±Ø© ØµØ§Ù„Ø­Ø© Ù„Ù„Ù…Ù‚Ø§Ù„"""
    if any(x in url for x in ['width=16', 'width=32', 'width=48', 'width=64', 'width=96', 'width=128', 'width=160']):
        return False
    
    exclude_keywords = ['avatar', 'author', 'profile', 'logo', 'icon', 'thumbnail', 'thumb']
    if any(keyword in url.lower() for keyword in exclude_keywords):
        return False
    
    return True

def scrape_article_images_with_alt(article_url):
    """ÙƒØ´Ø· Ø§Ù„ØµÙˆØ± Ù…Ø¹ Ù†ØµÙˆØµ alt Ù…Ù† Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…Ù‚Ø§Ù„"""
    print(f"--- ğŸ” ÙƒØ´Ø· ØµÙˆØ± Ø§Ù„Ù…Ù‚Ø§Ù„ Ø¨Ù€ Selenium Ù…Ù†: {article_url}")
    
    options = webdriver.ChromeOptions()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("window-size=1920,1080")
    options.add_argument("--disable-blink-features=AutomationControlled")
    
    service = ChromeService(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    
    stealth(driver,
            languages=["en-US", "en"],
            vendor="Google Inc.",
            platform="Win32",
            webgl_vendor="Intel Inc.",
            renderer="Intel Iris OpenGL Engine",
            fix_hairline=True)
    
    images_data = []  # Ø³Ù†Ø­ÙØ¸ Ø§Ù„ØµÙˆØ± Ù…Ø¹ alt text
    
    try:
        print("    â³ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©...")
        driver.get(article_url)
        
        wait = WebDriverWait(driver, 10)
        try:
            article_element = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "article.article")))
        except:
            try:
                article_element = wait.until(EC.presence_of_element_located((By.TAG_NAME, "article")))
            except:
                print("    âš ï¸ Ù„Ù… Ø£Ø¬Ø¯ Ø¹Ù†ØµØ± article")
                article_element = driver.find_element(By.TAG_NAME, "body")
        
        # Ø§Ù„ØªÙ…Ø±ÙŠØ± Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±
        driver.execute_script("arguments[0].scrollIntoView();", article_element)
        time.sleep(1)
        driver.execute_script("""
            var article = arguments[0];
            article.scrollTop = article.scrollHeight / 2;
        """, article_element)
        time.sleep(1)
        
        print("    ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ØµÙˆØ± Ùˆalt text...")
        
        img_elements = article_element.find_elements(By.TAG_NAME, "img")
        
        for img in img_elements:
            try:
                src = img.get_attribute("src")
                if not src:
                    src = img.get_attribute("data-src")
                if not src:
                    src = img.get_attribute("data-lazy-src")
                if not src:
                    src = img.get_attribute("data-original")
                if not src:
                    src = driver.execute_script("return arguments[0].currentSrc;", img)
                
                # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ alt text
                alt_text = img.get_attribute("alt")
                if not alt_text:
                    alt_text = img.get_attribute("title")
                if not alt_text:
                    alt_text = ""
                
                if src and "/assets/images/" in src:
                    clean_url = src
                    
                    if "/cdn-cgi/image/" in clean_url:
                        match = re.search(r'/assets/images/[^/\s]+', clean_url)
                        if match:
                            clean_url = f"https://{SITE_DOMAIN}" + match.group()
                    
                    if not clean_url.startswith("http"):
                        if clean_url.startswith("//"):
                            clean_url = "https:" + clean_url
                        elif clean_url.startswith("/"):
                            from urllib.parse import urljoin
                            clean_url = urljoin(article_url, clean_url)
                    
                    if is_valid_article_image(clean_url):
                        # Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø© Ù…Ø¹ alt text
                        image_exists = False
                        for img_data in images_data:
                            if img_data['url'] == clean_url:
                                image_exists = True
                                break
                        
                        if not image_exists:
                            images_data.append({
                                'url': clean_url,
                                'alt': alt_text
                            })
                            print(f"    âœ“ ØµÙˆØ±Ø©: {clean_url[:50]}... | Alt: {alt_text[:30]}...")
                        
            except Exception as e:
                continue
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ source tags
        source_elements = article_element.find_elements(By.TAG_NAME, "source")
        for source in source_elements:
            try:
                srcset = source.get_attribute("srcset")
                if srcset and "/assets/images/" in srcset:
                    urls_in_srcset = re.findall(r'([^\s,]+)', srcset)
                    for url in urls_in_srcset:
                        if "/assets/images/" in url and not any(x in url for x in ['width=48', 'width=96', 'width=160']):
                            if "/cdn-cgi/image/" in url:
                                match = re.search(r'/assets/images/[^/\s]+', url)
                                if match:
                                    url = f"https://{SITE_DOMAIN}" + match.group()
                            
                            if not url.startswith("http"):
                                from urllib.parse import urljoin
                                url = urljoin(article_url, url)
                            
                            if is_valid_article_image(url):
                                image_exists = False
                                for img_data in images_data:
                                    if img_data['url'] == url:
                                        image_exists = True
                                        break
                                
                                if not image_exists:
                                    # Ø¬Ø±Ø¨ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ alt Ù…Ù† Ø§Ù„Ù€ picture parent
                                    alt_text = ""
                                    try:
                                        picture = source.find_element(By.XPATH, "..")
                                        img_in_picture = picture.find_element(By.TAG_NAME, "img")
                                        alt_text = img_in_picture.get_attribute("alt") or ""
                                    except:
                                        pass
                                    
                                    images_data.append({
                                        'url': url,
                                        'alt': alt_text
                                    })
                                    print(f"    âœ“ ØµÙˆØ±Ø© Ù…Ù† srcset: {url[:50]}...")
            except:
                continue
        
        print(f"--- âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(images_data)} ØµÙˆØ±Ø© ØµØ§Ù„Ø­Ø© Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ù„")
        
    except Exception as e:
        print(f"--- âš ï¸ Ø®Ø·Ø£ ÙÙŠ Selenium: {e}")
    finally:
        driver.quit()
    
    return images_data

def get_best_images_for_article(article_url, rss_image=None):
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ ØµÙˆØ±ØªÙŠÙ† Ù…Ø¹ alt text"""
    scraped_images_data = scrape_article_images_with_alt(article_url)
    
    all_images_data = []
    
    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…ÙƒØ´ÙˆØ·Ø©
    all_images_data.extend(scraped_images_data)
    
    # Ø¥Ø¶Ø§ÙØ© ØµÙˆØ±Ø© RSS ÙƒØ§Ø­ØªÙŠØ§Ø·ÙŠ
    if rss_image and is_valid_article_image(rss_image):
        rss_exists = False
        for img_data in all_images_data:
            if img_data['url'] == rss_image:
                rss_exists = True
                break
        
        if not rss_exists:
            all_images_data.append({
                'url': rss_image,
                'alt': 'Featured recipe image'
            })
    
    # Ø§Ø®ØªÙŠØ§Ø± ØµÙˆØ±ØªÙŠÙ† Ù…Ø®ØªÙ„ÙØªÙŠÙ†
    if len(all_images_data) >= 2:
        image1_data = all_images_data[0]
        if len(all_images_data) >= 3:
            image2_data = all_images_data[2]
        else:
            image2_data = all_images_data[1]
    elif len(all_images_data) == 1:
        image1_data = image2_data = all_images_data[0]
    else:
        image1_data = image2_data = None
    
    return image1_data, image2_data

def rewrite_content_with_gemini(title, content_html, original_link, image1_alt="", image2_alt=""):
    if not GEMINI_API_KEY:
        print("!!! ØªØ­Ø°ÙŠØ±: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…ÙØªØ§Ø­ GEMINI_API_KEY.")
        return None

    print("--- ğŸ’¬ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Gemini API Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù‚Ø§Ù„ Ø§Ø­ØªØ±Ø§ÙÙŠ...")
    clean_content = re.sub('<[^<]+?>', ' ', content_html)
    
    # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª alt text Ø¥Ù„Ù‰ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª
    alt_info = ""
    if image1_alt:
        alt_info += f"\n- Image 1 description: {image1_alt}"
    if image2_alt and image2_alt != image1_alt:
        alt_info += f"\n- Image 2 description: {image2_alt}"
    
    prompt = """
    You are a professional SEO copywriter for Medium.
    Your task is to rewrite a recipe article for maximum engagement and SEO.

    **Original Data:**
    - Original Title: "%s"
    - Original Content: "%s"
    - Link to full recipe: "%s"%s

    **Requirements:**
    1. **New Title:** Create an engaging, SEO-optimized title (60-70 characters)
    2. **Article Body:** Write 600-700 words in clean HTML format
       - Start with a compelling introduction
       - Include practical tips and insights
       - Use headers (h2, h3) for structure
       - Add numbered or bulleted lists where appropriate
       - **IMPORTANT**: Use ONLY simple HTML tags (p, h2, h3, ul, ol, li, strong, em, br)
       - **DO NOT** use img, figure, or complex tags
       - Insert these EXACT placeholders AS WRITTEN:
         * INSERT_IMAGE_1_HERE (after the introduction paragraph)
         * INSERT_IMAGE_2_HERE (in the middle section of the article)
    3. **Call to Action:** End with a natural link to the original recipe
    4. **Tags:** Suggest 5 relevant Medium tags
    5. **Image Captions:** If you have image descriptions, create engaging captions that relate to them

    **Output Format:**
    Return ONLY a valid JSON object with these keys:
    - "new_title": The new title
    - "new_html_content": The HTML content with INSERT_IMAGE_1_HERE and INSERT_IMAGE_2_HERE placeholders
    - "tags": Array of 5 tags
    - "caption1": A short engaging caption for the first image (if applicable)
    - "caption2": A short engaging caption for the second image (if applicable)
    """ % (title, clean_content[:1500], original_link, alt_info)
    
    api_url = f'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={GEMINI_API_KEY}'
    headers = {'Content-Type': 'application/json'}
    data = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {"maxOutputTokens": 4096, "temperature": 0.7}
    }
    
    try:
        response = requests.post(api_url, headers=headers, data=json.dumps(data), timeout=180)
        response.raise_for_status()
        response_json = response.json()
        raw_text = response_json['candidates'][0]['content']['parts'][0]['text']
        
        json_match = re.search(r'\{.*\}', raw_text, re.DOTALL)
        if json_match:
            clean_json_str = json_match.group(0)
            result = json.loads(clean_json_str)
            print("--- âœ… ØªÙ… Ø§Ø³ØªÙ„Ø§Ù… Ù…Ù‚Ø§Ù„ Ù…Ø­Ø³Ù‘Ù† Ù…Ù† Gemini.")
            return {
                "title": result.get("new_title", title),
                "content": result.get("new_html_content", content_html),
                "tags": result.get("tags", []),
                "caption1": result.get("caption1", ""),
                "caption2": result.get("caption2", "")
            }
    except Exception as e:
        print(f"!!! Ø®Ø·Ø£ ÙÙŠ Gemini: {e}")
        return None

def prepare_html_with_multiple_images(content_html, image1_data, image2_data, original_link, caption1="", caption2=""):
    """Ø¥Ø¹Ø¯Ø§Ø¯ HTML Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù…Ø¹ Ø§Ù„ØµÙˆØ± Ùˆalt text"""
    
    print("--- ğŸ¨ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù…Ø¹ Ø§Ù„ØµÙˆØ±...")
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ HTML Ù„Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰
    if image1_data:
        alt1 = image1_data['alt'] or "Recipe preparation"
        # Ø¥Ø¶Ø§ÙØ© Ø§Ø³Ù… Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ù„Ù€ alt text
        full_alt1 = f"{alt1} | {SITE_DOMAIN}" if alt1 else f"Recipe image | {SITE_DOMAIN}"
        
        image1_html = f'<img src="{image1_data["url"]}" alt="{full_alt1}">'
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… caption Ù…Ù† Gemini Ø£Ùˆ alt text
        if caption1:
            image_caption1 = caption1
        elif image1_data['alt']:
            image_caption1 = f"{image1_data['alt']} | {SITE_DOMAIN}"
        else:
            image_caption1 = f"Step-by-step preparation | {SITE_DOMAIN}"
        
        image1_with_caption = f'{image1_html}<p><em>{image_caption1}</em></p>'
    else:
        image1_with_caption = ""
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ HTML Ù„Ù„ØµÙˆØ±Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©
    if image2_data:
        alt2 = image2_data['alt'] or "Final dish"
        # Ø¥Ø¶Ø§ÙØ© Ø§Ø³Ù… Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ù„Ù€ alt text
        full_alt2 = f"{alt2} | {SITE_DOMAIN}" if alt2 else f"Recipe result | {SITE_DOMAIN}"
        
        image2_html = f'<img src="{image2_data["url"]}" alt="{full_alt2}">'
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… caption Ù…Ù† Gemini Ø£Ùˆ alt text
        if caption2:
            image_caption2 = caption2
        elif image2_data['alt'] and image2_data['alt'] != image1_data.get('alt', ''):
            image_caption2 = f"{image2_data['alt']} | {SITE_DOMAIN}"
        elif image2_data['url'] == image1_data.get('url', ''):
            image_caption2 = f"Another view of this delicious recipe | {SITE_DOMAIN}"
        else:
            image_caption2 = f"The final result - absolutely delicious! | {SITE_DOMAIN}"
        
        image2_with_caption = f'{image2_html}<p><em>{image_caption2}</em></p>'
    else:
        image2_with_caption = ""
    
    # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª
    content_html = content_html.replace("INSERT_IMAGE_1_HERE", image1_with_caption)
    content_html = content_html.replace("INSERT_IMAGE_2_HERE", image2_with_caption)
    
    # Ø¥Ø¶Ø§ÙØ© Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ØµØ¯Ø±
    call_to_action = f'<br><p><strong>For the complete recipe with step-by-step instructions and tips, visit <a href="{original_link}" rel="noopener" target="_blank">{SITE_DOMAIN}</a>.</strong></p>'
    
    return content_html + call_to_action

def main():
    print(f"--- Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±ÙˆØ¨ÙˆØª Ø§Ù„Ù†Ø§Ø´Ø± v28 (Ù…Ø¹ Alt Text) Ù„Ù…ÙˆÙ‚Ø¹ {SITE_DOMAIN} ---")
    post_to_publish = get_next_post_to_publish()
    if not post_to_publish:
        print(">>> Ø§Ù„Ù†ØªÙŠØ¬Ø©: Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù‚Ø§Ù„Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©.")
        return

    original_title = post_to_publish.title
    original_link = post_to_publish.link
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØµÙˆØ±Ø© RSS
    rss_image = extract_image_url_from_entry(post_to_publish)
    if rss_image:
        print(f"--- ğŸ“· ØµÙˆØ±Ø© RSS Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©: {rss_image[:80]}...")
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ± Ù…Ø¹ alt text
    image1_data, image2_data = get_best_images_for_article(original_link, rss_image)
    
    if image1_data:
        print(f"--- ğŸ–¼ï¸ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: {image1_data['url'][:60]}...")
        if image1_data['alt']:
            print(f"      Alt: {image1_data['alt'][:50]}...")
    if image2_data:
        print(f"--- ğŸ–¼ï¸ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: {image2_data['url'][:60]}...")
        if image2_data['alt']:
            print(f"      Alt: {image2_data['alt'][:50]}...")
    
    if not image1_data:
        print("--- âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØµÙˆØ± ØµØ§Ù„Ø­Ø© Ù„Ù„Ù…Ù‚Ø§Ù„!")
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø£ØµÙ„ÙŠ
    original_content_html = ""
    if 'content' in post_to_publish and post_to_publish.content:
        original_content_html = post_to_publish.content[0].value
    else:
        original_content_html = post_to_publish.summary

    # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Gemini
    image1_alt = image1_data['alt'] if image1_data else ""
    image2_alt = image2_data['alt'] if image2_data else ""
    
    rewritten_data = rewrite_content_with_gemini(
        original_title, original_content_html, original_link, image1_alt, image2_alt
    )
    
    if rewritten_data:
        final_title = rewritten_data["title"]
        ai_content = rewritten_data["content"]
        ai_tags = rewritten_data.get("tags", [])
        caption1 = rewritten_data.get("caption1", "")
        caption2 = rewritten_data.get("caption2", "")
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        full_html_content = prepare_html_with_multiple_images(
            ai_content, image1_data, image2_data, original_link, caption1, caption2
        )
        print("--- âœ… ØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…ÙØ­Ø³Ù‘Ù† Ù…Ø¹ Ø§Ù„ØµÙˆØ± Ùˆalt text.")
    else:
        print("--- âš ï¸ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø£ØµÙ„ÙŠ.")
        final_title = original_title
        ai_tags = []
        
        if image1_data:
            alt1 = f"{image1_data['alt']} | {SITE_DOMAIN}" if image1_data['alt'] else f"Recipe image | {SITE_DOMAIN}"
            image1_html = f'<img src="{image1_data["url"]}" alt="{alt1}">'
            caption1 = f"<p><em>{alt1}</em></p>"
        else:
            image1_html = ""
            caption1 = ""
        
        if image2_data and image2_data['url'] != image1_data.get('url', ''):
            alt2 = f"{image2_data['alt']} | {SITE_DOMAIN}" if image2_data['alt'] else f"Recipe detail | {SITE_DOMAIN}"
            image2_html = f'<br><img src="{image2_data["url"]}" alt="{alt2}">'
            caption2 = f"<p><em>{alt2}</em></p>"
        else:
            image2_html = ""
            caption2 = ""
        
        link_html = f'<br><p><em>For the full recipe, visit <a href="{original_link}" rel="noopener" target="_blank">{SITE_DOMAIN}</a>.</em></p>'
        full_html_content = image1_html + caption1 + original_content_html + image2_html + caption2 + link_html

    # --- Ø§Ù„Ù†Ø´Ø± Ø¹Ù„Ù‰ Medium ---
    sid_cookie = os.environ.get("MEDIUM_SID_COOKIE")
    uid_cookie = os.environ.get("MEDIUM_UID_COOKIE")
    
    if not sid_cookie or not uid_cookie:
        print("!!! Ø®Ø·Ø£: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙˆÙƒÙŠØ².")
        return

    options = webdriver.ChromeOptions()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("window-size=1920,1080")
    
    service = ChromeService(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)

    stealth(driver, 
            languages=["en-US", "en"], 
            vendor="Google Inc.", 
            platform="Win32", 
            webgl_vendor="Intel Inc.", 
            renderer="Intel Iris OpenGL Engine", 
            fix_hairline=True)
    
    try:
        print("--- 2. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¬Ù„Ø³Ø©...")
        driver.get("https://medium.com/")
        driver.add_cookie({"name": "sid", "value": sid_cookie, "domain": ".medium.com"})
        driver.add_cookie({"name": "uid", "value": uid_cookie, "domain": ".medium.com"})
        
        print("--- 3. Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ø¥Ù„Ù‰ Ù…Ø­Ø±Ø± Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª...")
        driver.get("https://medium.com/new-story")
        
        wait = WebDriverWait(driver, 30)
        
        print("--- 4. ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¹Ù†ÙˆØ§Ù†...")
        title_field = wait.until(EC.element_to_be_clickable(
            (By.CSS_SELECTOR, 'h3[data-testid="editorTitleParagraph"]')
        ))
        title_field.click()
        title_field.send_keys(final_title)
        
        print("--- 5. Ø¥Ø¯Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ø¹ Ø§Ù„ØµÙˆØ±...")
        story_field = wait.until(EC.element_to_be_clickable(
            (By.CSS_SELECTOR, 'p[data-testid="editorParagraphText"]')
        ))
        story_field.click()
        
        js_script = """
        const html = arguments[0];
        const blob = new Blob([html], { type: 'text/html' });
        const item = new ClipboardItem({ 'text/html': blob });
        navigator.clipboard.write([item]);
        """
        driver.execute_script(js_script, full_html_content)
        story_field.send_keys(Keys.CONTROL, 'v')
        
        print("--- â³ Ø§Ù†ØªØ¸Ø§Ø± Ø±ÙØ¹ Ø§Ù„ØµÙˆØ±...")
        time.sleep(12)
        
        print("--- 6. Ø¨Ø¯Ø¡ Ø§Ù„Ù†Ø´Ø±...")
        publish_button = wait.until(EC.element_to_be_clickable(
            (By.CSS_SELECTOR, 'button[data-action="show-prepublish"]')
        ))
        publish_button.click()
        
        print("--- 7. Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙˆØ³ÙˆÙ…...")
        if ai_tags:
            try:
                tags_input = wait.until(EC.presence_of_element_located(
                    (By.CSS_SELECTOR, 'div[data-testid="publishTopicsInput"]')
                ))
                tags_input.click()
                
                for tag in ai_tags[:5]:
                    tags_input.send_keys(tag)
                    time.sleep(0.5)
                    tags_input.send_keys(Keys.ENTER)
                    time.sleep(1)
                print(f"--- ØªÙ…Øª Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙˆØ³ÙˆÙ…: {', '.join(ai_tags[:5])}")
            except:
                print("--- ØªØ®Ø·ÙŠ Ø§Ù„ÙˆØ³ÙˆÙ…")
        
        print("--- 8. Ø§Ù„Ù†Ø´Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ...")
        publish_now_button = wait.until(EC.element_to_be_clickable(
            (By.CSS_SELECTOR, 'button[data-testid="publishConfirmButton"]')
        ))
        time.sleep(2)
        driver.execute_script("arguments[0].click();", publish_now_button)
        
        print("--- 9. Ø§Ù†ØªØ¸Ø§Ø± Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Ø´Ø±...")
        time.sleep(15)
        
        add_posted_link(post_to_publish.link)
        print(f">>> ğŸ‰ğŸ‰ğŸ‰ ØªÙ… Ù†Ø´Ø± Ø§Ù„Ù…Ù‚Ø§Ù„ Ø¨Ù†Ø¬Ø§Ø­ Ø¹Ù„Ù‰ {SITE_DOMAIN}! ğŸ‰ğŸ‰ğŸ‰")
        
    except Exception as e:
        print(f"!!! Ø®Ø·Ø£: {e}")
        driver.save_screenshot("error_screenshot.png")
        with open("error_page_source.html", "w", encoding="utf-8") as f:
            f.write(driver.page_source)
        raise e
    finally:
        driver.quit()
        print("--- ØªÙ… Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø±ÙˆØ¨ÙˆØª ---")

if __name__ == "__main__":
    main()
