import feedparser
import os
import time
import re
import requests
import json
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium_stealth import stealth

# --- Ø¨Ø±Ù…Ø¬Ø© ahmed si - Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…ÙØ­Ø³Ù‘Ù†Ø© v25 ---

RSS_URL = "https://Fastyummyfood.com/feed"
POSTED_LINKS_FILE = "posted_links.txt"
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")

def get_posted_links():
    if not os.path.exists(POSTED_LINKS_FILE): return set()
    with open(POSTED_LINKS_FILE, "r", encoding='utf-8') as f: return set(line.strip() for line in f)

def add_posted_link(link):
    with open(POSTED_LINKS_FILE, "a", encoding='utf-8') as f: f.write(link + "\n")

def get_next_post_to_publish():
    print(f"--- 1. Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ù‚Ø§Ù„Ø§Øª ÙÙŠ: {RSS_URL}")
    feed = feedparser.parse(RSS_URL)
    if not feed.entries: return None
    print(f"--- ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(feed.entries)} Ù…Ù‚Ø§Ù„Ø§Øª.")
    posted_links = get_posted_links()
    for entry in reversed(feed.entries):
        if entry.link not in posted_links:
            print(f">>> ØªÙ… ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù‚Ø§Ù„: {entry.title}")
            return entry
    return None

def extract_image_url_from_entry(entry):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙˆÙ„ ØµÙˆØ±Ø© Ù…Ù† RSS feed"""
    if hasattr(entry, 'media_content') and entry.media_content:
        for media in entry.media_content:
            if 'url' in media and media.get('medium') == 'image': return media['url']
    if hasattr(entry, 'enclosures') and entry.enclosures:
        for enclosure in entry.enclosures:
            if 'href' in enclosure and 'image' in enclosure.get('type', ''): return enclosure.href
    content_html = ""
    if 'content' in entry and entry.content: content_html = entry.content[0].value
    else: content_html = entry.summary
    match = re.search(r'<img[^>]+src="([^">]+)"', content_html)
    if match: return match.group(1)
    return None

def make_absolute_url(url, base_url):
    """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù†Ø³Ø¨ÙŠØ© Ø¥Ù„Ù‰ Ù…Ø·Ù„Ù‚Ø©"""
    if not url:
        return None
    
    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ù…Ø·Ù„Ù‚Ø§Ù‹ Ø¨Ø§Ù„ÙØ¹Ù„
    if url.startswith('http://') or url.startswith('https://'):
        return url
    
    # Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ¨Ø¯Ø£ Ø¨Ù€ //
    if url.startswith('//'):
        return 'https:' + url
    
    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø±Ø§Ø¨Ø· Ù†Ø³Ø¨ÙŠ
    return urljoin(base_url, url)

def scrape_article_images_enhanced(article_url):
    """ÙƒØ´Ø· Ù…Ø­Ø³Ù‘Ù† Ù„Ù„ØµÙˆØ± Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ù„ - ÙŠØ¨Ø­Ø« Ø¨Ø·Ø±Ù‚ Ù…ØªØ¹Ø¯Ø¯Ø©"""
    print(f"--- ğŸ” Ø¬Ø§Ø±ÙŠ ÙƒØ´Ø· Ø§Ù„ØµÙˆØ± Ù…Ù†: {article_url}")
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }
        
        response = requests.get(article_url, headers=headers, timeout=20)
        response.raise_for_status()
        html_content = response.text
        
        images = []
        
        # Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1: Ø§Ù„Ø¨Ø­Ø« Ø¨Ù€ regex Ø¹Ù† Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØµÙˆØ± Ù…Ø¨Ø§Ø´Ø±Ø©
        print("    ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØµÙˆØ± Ø¨Ù€ regex...")
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙŠ Ø±Ø§Ø¨Ø· ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ /assets/images/
        pattern1 = r'["\']([^"\']*?/assets/images/[^"\']+\.(?:jpg|jpeg|png|gif|webp))["\']'
        matches1 = re.findall(pattern1, html_content, re.IGNORECASE)
        for match in matches1:
            clean_url = match.split('?')[0]  # Ø¥Ø²Ø§Ù„Ø© query parameters
            absolute_url = make_absolute_url(clean_url, article_url)
            if absolute_url and absolute_url not in images:
                images.append(absolute_url)
                print(f"    âœ“ ÙˆØ¬Ø¯Øª ØµÙˆØ±Ø© (regex): {absolute_url[:60]}...")
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ØµÙˆØ± ÙÙŠ src Ø£Ùˆ data-src Ø£Ùˆ srcset
        pattern2 = r'(?:src|data-src|data-lazy-src|data-original)=["\']([^"\']+\.(?:jpg|jpeg|png|gif|webp))["\']'
        matches2 = re.findall(pattern2, html_content, re.IGNORECASE)
        for match in matches2:
            if '/assets/images/' in match or 'fastyummyfood' in match.lower():
                clean_url = match.split('?')[0]
                absolute_url = make_absolute_url(clean_url, article_url)
                if absolute_url and absolute_url not in images:
                    images.append(absolute_url)
                    print(f"    âœ“ ÙˆØ¬Ø¯Øª ØµÙˆØ±Ø© (src): {absolute_url[:60]}...")
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ srcset
        pattern3 = r'srcset=["\']([^"\']+)["\']'
        srcset_matches = re.findall(pattern3, html_content, re.IGNORECASE)
        for srcset in srcset_matches:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† srcset
            urls_in_srcset = re.findall(r'([^\s,]+\.(?:jpg|jpeg|png|gif|webp))', srcset, re.IGNORECASE)
            for url in urls_in_srcset:
                if '/assets/images/' in url or 'fastyummyfood' in url.lower():
                    clean_url = url.split('?')[0]
                    absolute_url = make_absolute_url(clean_url, article_url)
                    if absolute_url and absolute_url not in images:
                        images.append(absolute_url)
                        print(f"    âœ“ ÙˆØ¬Ø¯Øª ØµÙˆØ±Ø© (srcset): {absolute_url[:60]}...")
        
        # Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 2: Ø§Ø³ØªØ®Ø¯Ø§Ù… BeautifulSoup ÙƒØ®ÙŠØ§Ø± Ø¥Ø¶Ø§ÙÙŠ
        if len(images) < 2:
            print("    ğŸ” Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥Ø¶Ø§ÙÙŠØ© Ø¨Ù€ BeautifulSoup...")
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ ÙƒÙ„ Ø§Ù„ØµÙØ­Ø©
            for tag in soup.find_all(['img', 'source', 'picture']):
                # Ø¬Ø±Ø¨ ÙƒÙ„ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù…Ù…ÙƒÙ†Ø©
                possible_attrs = ['src', 'data-src', 'data-lazy-src', 'data-original', 
                                'data-srcset', 'srcset', 'data-image', 'data-bg']
                
                for attr in possible_attrs:
                    value = tag.get(attr)
                    if value:
                        # Ø¥Ø°Ø§ ÙƒØ§Ù† srcsetØŒ Ø§Ø³ØªØ®Ø±Ø¬ Ø£ÙˆÙ„ Ø±Ø§Ø¨Ø·
                        if 'srcset' in attr:
                            first_url = re.search(r'([^\s,]+)', value)
                            if first_url:
                                value = first_url.group(1)
                        
                        # ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ø±Ø§Ø¨Ø· ØµÙˆØ±Ø©
                        if any(ext in value.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                            clean_url = value.split('?')[0]
                            absolute_url = make_absolute_url(clean_url, article_url)
                            if absolute_url and absolute_url not in images:
                                images.append(absolute_url)
                                print(f"    âœ“ ÙˆØ¬Ø¯Øª ØµÙˆØ±Ø© (soup): {absolute_url[:60]}...")
        
        # Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 3: Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙŠ Ø±Ø§Ø¨Ø· ÙŠØ´Ø¨Ù‡ Ø±Ø§Ø¨Ø· ØµÙˆØ±Ø©
        if len(images) < 2:
            print("    ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø´Ø§Ù…Ù„ Ø¹Ù† Ø£ÙŠ Ø±ÙˆØ§Ø¨Ø· ØµÙˆØ±...")
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙŠ Ø´ÙŠØ¡ ÙŠØ´Ø¨Ù‡ Ø±Ø§Ø¨Ø· ØµÙˆØ±Ø©
            all_image_pattern = r'https?://[^\s"\'<>]+\.(?:jpg|jpeg|png|gif|webp)'
            all_matches = re.findall(all_image_pattern, html_content, re.IGNORECASE)
            for match in all_matches:
                if 'fastyummyfood' in match.lower() and match not in images:
                    images.append(match)
                    print(f"    âœ“ ÙˆØ¬Ø¯Øª ØµÙˆØ±Ø© (Ø¹Ø§Ù…): {match[:60]}...")
                    if len(images) >= 5:  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 5 ØµÙˆØ±
                        break
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ØªØ±ØªÙŠØ¨
        unique_images = []
        seen = set()
        for img in images:
            if img not in seen:
                unique_images.append(img)
                seen.add(img)
        
        print(f"--- âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(unique_images)} ØµÙˆØ±Ø© ÙØ±ÙŠØ¯Ø© ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ù„")
        return unique_images
        
    except Exception as e:
        print(f"--- âš ï¸ ÙØ´Ù„ ÙƒØ´Ø· Ø§Ù„ØµÙˆØ±: {e}")
        return []

def get_best_images_for_article(article_url, rss_image=None):
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ ØµÙˆØ±ØªÙŠÙ† Ù„Ù„Ù…Ù‚Ø§Ù„"""
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø© Ù„Ù„ÙƒØ´Ø·
    scraped_images = scrape_article_images_enhanced(article_url)
    
    all_images = []
    
    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…ÙƒØ´ÙˆØ·Ø© Ø£ÙˆÙ„Ø§Ù‹ (Ù„Ù‡Ø§ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©)
    all_images.extend(scraped_images)
    
    # Ø¥Ø¶Ø§ÙØ© ØµÙˆØ±Ø© RSS ÙƒØ®ÙŠØ§Ø± Ø§Ø­ØªÙŠØ§Ø·ÙŠ
    if rss_image and rss_image not in all_images:
        all_images.append(rss_image)
    
    # Ø§Ø®ØªÙŠØ§Ø± ØµÙˆØ±ØªÙŠÙ† Ù…Ø®ØªÙ„ÙØªÙŠÙ†
    if len(all_images) >= 2:
        image1 = all_images[0]
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØµÙˆØ±Ø© Ù…Ø®ØªÙ„ÙØ© Ù„Ù„Ù…ÙˆØ¶Ø¹ Ø§Ù„Ø«Ø§Ù†ÙŠ
        if len(all_images) >= 3:
            image2 = all_images[2]  # ØªØ®Ø·ÙŠ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ© Ù„Ù„ØªÙ†ÙˆØ¹
        else:
            image2 = all_images[1]
    elif len(all_images) == 1:
        # Ø¥Ø°Ø§ ÙˆØ¬Ø¯Ù†Ø§ ØµÙˆØ±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·ØŒ Ø§Ø³ØªØ®Ø¯Ù…Ù‡Ø§ Ù…Ø±ØªÙŠÙ†
        image1 = image2 = all_images[0]
    else:
        # Ù„Ø§ ØªÙˆØ¬Ø¯ ØµÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø·Ù„Ø§Ù‚
        image1 = image2 = None
    
    return image1, image2

def rewrite_content_with_gemini(title, content_html, original_link):
    if not GEMINI_API_KEY:
        print("!!! ØªØ­Ø°ÙŠØ±: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…ÙØªØ§Ø­ GEMINI_API_KEY.")
        return None

    print("--- ğŸ’¬ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Gemini API Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù‚Ø§Ù„ Ø§Ø­ØªØ±Ø§ÙÙŠ...")
    clean_content = re.sub('<[^<]+?>', ' ', content_html)
    
    prompt = """
    You are a professional SEO copywriter for Medium.
    Your task is to rewrite a recipe article for maximum engagement and SEO.

    **Original Data:**
    - Original Title: "%s"
    - Original Content: "%s"
    - Link to full recipe: "%s"

    **Requirements:**
    1. **New Title:** Create an engaging, SEO-optimized title (60-70 characters)
    2. **Article Body:** Write 600-700 words in clean HTML format
       - Start with a compelling introduction
       - Include practical tips and insights
       - Use headers (h2, h3) for structure
       - Add numbered or bulleted lists where appropriate
       - **IMPORTANT**: Use ONLY simple HTML tags (p, h2, h3, ul, ol, li, strong, em, br)
       - **DO NOT** use img, figure, or complex tags
       - Insert these EXACT placeholders AS WRITTEN:
         * INSERT_IMAGE_1_HERE (after the introduction paragraph)
         * INSERT_IMAGE_2_HERE (in the middle section of the article)
    3. **Call to Action:** End with a natural link to the original recipe
    4. **Tags:** Suggest 5 relevant Medium tags

    **Output Format:**
    Return ONLY a valid JSON object with these keys:
    - "new_title": The new title
    - "new_html_content": The HTML content with INSERT_IMAGE_1_HERE and INSERT_IMAGE_2_HERE placeholders
    - "tags": Array of 5 tags
    """ % (title, clean_content[:1500], original_link)
    
    api_url = f'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={GEMINI_API_KEY}'
    headers = {'Content-Type': 'application/json'}
    data = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {"maxOutputTokens": 4096, "temperature": 0.7}
    }
    
    try:
        response = requests.post(api_url, headers=headers, data=json.dumps(data), timeout=180)
        response.raise_for_status()
        response_json = response.json()
        raw_text = response_json['candidates'][0]['content']['parts'][0]['text']
        
        json_match = re.search(r'\{.*\}', raw_text, re.DOTALL)
        if json_match:
            clean_json_str = json_match.group(0)
            result = json.loads(clean_json_str)
            print("--- âœ… ØªÙ… Ø§Ø³ØªÙ„Ø§Ù… Ù…Ù‚Ø§Ù„ Ù…Ø­Ø³Ù‘Ù† Ù…Ù† Gemini.")
            return {
                "title": result.get("new_title", title),
                "content": result.get("new_html_content", content_html),
                "tags": result.get("tags", [])
            }
    except Exception as e:
        print(f"!!! Ø®Ø·Ø£ ÙÙŠ Gemini: {e}")
        return None

def prepare_html_with_multiple_images(content_html, image1, image2, original_link):
    """Ø¥Ø¹Ø¯Ø§Ø¯ HTML Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù…Ø¹ ØµÙˆØ±ØªÙŠÙ† Ù…Ø®ØªÙ„ÙØªÙŠÙ†"""
    
    print("--- ğŸ¨ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù…Ø¹ Ø§Ù„ØµÙˆØ±...")
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ HTML Ù„Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰
    if image1:
        image1_html = f'<img src="{image1}" alt="Recipe preparation">'
        image1_with_caption = f'{image1_html}<p><em>Step-by-step preparation process</em></p>'
    else:
        image1_with_caption = ""
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ HTML Ù„Ù„ØµÙˆØ±Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©  
    if image2:
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØµÙ Ù…Ø®ØªÙ„Ù Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù†ÙØ³ Ø§Ù„ØµÙˆØ±Ø©
        if image2 == image1:
            caption2 = "Another view of this delicious recipe"
        else:
            caption2 = "The delicious final result!"
        image2_html = f'<img src="{image2}" alt="Final dish">'
        image2_with_caption = f'{image2_html}<p><em>{caption2}</em></p>'
    else:
        image2_with_caption = ""
    
    # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª
    placeholders = [
        ("INSERT_IMAGE_1_HERE", image1_with_caption),
        ("INSERT_IMAGE_2_HERE", image2_with_caption),
    ]
    
    for placeholder, replacement in placeholders:
        if placeholder in content_html:
            content_html = content_html.replace(placeholder, replacement)
            print(f"    âœ“ ØªÙ… Ø§Ø³ØªØ¨Ø¯Ø§Ù„: {placeholder}")
    
    # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª ÙˆÙ„Ø¯ÙŠÙ†Ø§ ØµÙˆØ±ØŒ Ø£Ø¶ÙÙ‡Ø§
    if image1 and "INSERT_IMAGE" not in content_html and "<img" not in content_html:
        print("    âš ï¸ Ù„Ù… Ø£Ø¬Ø¯ Ø§Ù„Ø¹Ù„Ø§Ù…Ø§ØªØŒ Ø³Ø£Ø¶Ø¹ Ø§Ù„ØµÙˆØ± ÙÙŠ Ù…ÙˆØ§Ø¶Ø¹ ØªÙ„Ù‚Ø§Ø¦ÙŠØ©")
        if "</p>" in content_html:
            first_p_end = content_html.find("</p>") + 4
            content_html = content_html[:first_p_end] + image1_with_caption + content_html[first_p_end:]
        
        if image2 and image2 != image1:
            mid_point = len(content_html) // 2
            next_p = content_html.find("</p>", mid_point)
            if next_p != -1:
                content_html = content_html[:next_p+4] + image2_with_caption + content_html[next_p+4:]
    
    # Ø¥Ø¶Ø§ÙØ© Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ØµØ¯Ø±
    site_name = "Fastyummyfood.com"
    call_to_action = f'<br><p><strong>For the complete recipe with detailed instructions, visit <a href="{original_link}" rel="noopener" target="_blank">{site_name}</a>.</strong></p>'
    
    return content_html + call_to_action

def main():
    print("--- Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±ÙˆØ¨ÙˆØª Ø§Ù„Ù†Ø§Ø´Ø± v25 (ÙƒØ´Ø· Ù…Ø­Ø³Ù‘Ù† Ù„Ù„ØµÙˆØ±) ---")
    post_to_publish = get_next_post_to_publish()
    if not post_to_publish:
        print(">>> Ø§Ù„Ù†ØªÙŠØ¬Ø©: Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù‚Ø§Ù„Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©.")
        return

    original_title = post_to_publish.title
    original_link = post_to_publish.link
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØµÙˆØ±Ø© RSS Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    rss_image = extract_image_url_from_entry(post_to_publish)
    if rss_image:
        print(f"--- ğŸ“· ØµÙˆØ±Ø© RSS: {rss_image[:80]}...")
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ ØµÙˆØ±ØªÙŠÙ† Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ù„
    image1, image2 = get_best_images_for_article(original_link, rss_image)
    
    if image1:
        print(f"--- ğŸ–¼ï¸ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ù„Ù„Ù†Ø´Ø±: {image1[:80]}...")
    if image2:
        print(f"--- ğŸ–¼ï¸ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ© Ù„Ù„Ù†Ø´Ø±: {image2[:80]}...")
    
    if not image1 and not image2:
        print("--- âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ ØµÙˆØ±!")
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø£ØµÙ„ÙŠ
    original_content_html = ""
    if 'content' in post_to_publish and post_to_publish.content:
        original_content_html = post_to_publish.content[0].value
    else:
        original_content_html = post_to_publish.summary

    # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Gemini
    rewritten_data = rewrite_content_with_gemini(
        original_title, original_content_html, original_link
    )
    
    if rewritten_data:
        final_title = rewritten_data["title"]
        ai_content = rewritten_data["content"]
        ai_tags = rewritten_data.get("tags", [])
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù…Ø¹ Ø§Ù„ØµÙˆØ±
        full_html_content = prepare_html_with_multiple_images(
            ai_content, image1, image2, original_link
        )
        print("--- âœ… ØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…ÙØ­Ø³Ù‘Ù† Ù…Ø¹ Ø§Ù„ØµÙˆØ±.")
    else:
        print("--- âš ï¸ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø£ØµÙ„ÙŠ.")
        final_title = original_title
        ai_tags = []
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø¨Ø³ÙŠØ·Ø©
        if image1:
            image1_html = f'<img src="{image1}" alt="Recipe image">'
        else:
            image1_html = ""
        
        if image2 and image2 != image1:
            image2_html = f'<br><img src="{image2}" alt="Recipe detail">'
        else:
            image2_html = ""
        
        call_to_action = "For the full recipe, visit"
        link_html = f'<br><p><em>{call_to_action} <a href="{original_link}" rel="noopener" target="_blank">Fastyummyfood.com</a>.</em></p>'
        full_html_content = image1_html + original_content_html + image2_html + link_html

    # --- Ø¨Ø¯Ø§ÙŠØ© Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ù†Ø´Ø± Ø¹Ù„Ù‰ Medium ---
    sid_cookie = os.environ.get("MEDIUM_SID_COOKIE")
    uid_cookie = os.environ.get("MEDIUM_UID_COOKIE")
    
    if not sid_cookie or not uid_cookie:
        print("!!! Ø®Ø·Ø£: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙˆÙƒÙŠØ².")
        return

    options = webdriver.ChromeOptions()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("window-size=1920,1080")
    options.add_argument("--disable-gpu")
    options.add_argument("--disable-web-security")
    options.add_argument("--disable-features=VizDisplayCompositor")
    
    service = ChromeService(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)

    stealth(driver, 
            languages=["en-US", "en"], 
            vendor="Google Inc.", 
            platform="Win32", 
            webgl_vendor="Intel Inc.", 
            renderer="Intel Iris OpenGL Engine", 
            fix_hairline=True)
    
    try:
        print("--- 2. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¬Ù„Ø³Ø©...")
        driver.get("https://medium.com/")
        driver.add_cookie({"name": "sid", "value": sid_cookie, "domain": ".medium.com"})
        driver.add_cookie({"name": "uid", "value": uid_cookie, "domain": ".medium.com"})
        
        print("--- 3. Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ø¥Ù„Ù‰ Ù…Ø­Ø±Ø± Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª...")
        driver.get("https://medium.com/new-story")
        
        wait = WebDriverWait(driver, 30)
        
        print("--- 4. ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¹Ù†ÙˆØ§Ù†...")
        title_field = wait.until(EC.element_to_be_clickable(
            (By.CSS_SELECTOR, 'h3[data-testid="editorTitleParagraph"]')
        ))
        title_field.click()
        title_field.send_keys(final_title)
        
        print("--- 5. Ø¥Ø¯Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ø¹ Ø§Ù„ØµÙˆØ±...")
        story_field = wait.until(EC.element_to_be_clickable(
            (By.CSS_SELECTOR, 'p[data-testid="editorParagraphText"]')
        ))
        story_field.click()
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù„ØµÙ‚ Ø§Ù„Ù…ÙØ«Ø¨ØªØ©
        js_script = """
        const html = arguments[0];
        const blob = new Blob([html], { type: 'text/html' });
        const item = new ClipboardItem({ 'text/html': blob });
        navigator.clipboard.write([item]);
        """
        driver.execute_script(js_script, full_html_content)
        story_field.send_keys(Keys.CONTROL, 'v')
        
        # Ø§Ù†ØªØ¸Ø§Ø± Ø£Ø·ÙˆÙ„ Ù„Ø±ÙØ¹ Ø§Ù„ØµÙˆØ±
        print("--- â³ Ø§Ù†ØªØ¸Ø§Ø± Ø±ÙØ¹ Ø§Ù„ØµÙˆØ± Ø¹Ù„Ù‰ Medium...")
        time.sleep(12)
        
        print("--- 6. Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ù†Ø´Ø±...")
        publish_button = wait.until(EC.element_to_be_clickable(
            (By.CSS_SELECTOR, 'button[data-action="show-prepublish"]')
        ))
        publish_button.click()
        
        print("--- 7. Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙˆØ³ÙˆÙ…...")
        if ai_tags:
            try:
                tags_input = wait.until(EC.presence_of_element_located(
                    (By.CSS_SELECTOR, 'div[data-testid="publishTopicsInput"]')
                ))
                tags_input.click()
                
                for tag in ai_tags[:5]:
                    tags_input.send_keys(tag)
                    time.sleep(0.5)
                    tags_input.send_keys(Keys.ENTER)
                    time.sleep(1)
                print(f"--- ØªÙ…Øª Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙˆØ³ÙˆÙ…: {', '.join(ai_tags[:5])}")
            except:
                print("--- ØªØ®Ø·ÙŠ Ø§Ù„ÙˆØ³ÙˆÙ… (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)")
        
        print("--- 8. Ø§Ù„Ù†Ø´Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ...")
        publish_now_button = wait.until(EC.element_to_be_clickable(
            (By.CSS_SELECTOR, 'button[data-testid="publishConfirmButton"]')
        ))
        time.sleep(2)
        driver.execute_script("arguments[0].click();", publish_now_button)
        
        print("--- 9. Ø§Ù†ØªØ¸Ø§Ø± Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Ø´Ø±...")
        time.sleep(15)
        
        # Ø­ÙØ¸ Ø§Ù„Ø±Ø§Ø¨Ø· ÙƒÙ…Ù†Ø´ÙˆØ±
        add_posted_link(post_to_publish.link)
        print(">>> ğŸ‰ğŸ‰ğŸ‰ ØªÙ… Ù†Ø´Ø± Ø§Ù„Ù…Ù‚Ø§Ù„ Ø¨Ù†Ø¬Ø§Ø­ Ù…Ø¹ Ø§Ù„ØµÙˆØ±! ğŸ‰ğŸ‰ğŸ‰")
        
    except Exception as e:
        print(f"!!! Ø­Ø¯Ø« Ø®Ø·Ø£: {e}")
        driver.save_screenshot("error_screenshot.png")
        with open("error_page_source.html", "w", encoding="utf-8") as f:
            f.write(driver.page_source)
        raise e
    finally:
        driver.quit()
        print("--- ØªÙ… Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø±ÙˆØ¨ÙˆØª ---")

if __name__ == "__main__":
    main()
